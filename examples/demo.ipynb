{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from paradigm_client.remote_model import RemoteModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ.get(\"HOST\", None)\n",
    "assert host is not None, \"{HOST} env var is not properly set. Run `export HOST=<value>` in your shell or add it to your `.bashrc`\"\n",
    "api_key = os.environ.get(\"PARADIGM_API_KEY\", None)\n",
    "assert host is not None, \"{PARADIGM_API_KEY} env var is not properly set. Run `export PARADIGM_API_KEY=<value>` in your shell or add it to your `.bashrc`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8119f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RemoteModel(\n",
    "    f\"{host}\", \n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-API-KEY\": api_key,\n",
    "        \"X-Model\": \"llm-mini\",\n",
    "    },\n",
    "    timeout_s=120\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Do you ever feel like you're talking to a wall when trying to communicate with large language models (LLM) ? As helpful as these models can be, communicating with them in the same way we communicate with humans can sometimes be a bit frustrating. But don't worry, there are alternative approaches that can help you get the most out of these powerful tools.\n",
    "In this blog, we'll explore why asking questions in the same way we would ask a human may not always be the best approach when working with large language models. We'll also discuss two alternative approaches that can yield better results and help you unlock new possibilities in natural language processing.\n",
    "\n",
    "So, why is it sometimes difficult to communicate with large language models? Well, most of these models are pre-trained and have been exposed to a ton of human-written documents, such as books, blogs, code, and more. While they may have encountered some dialogue in these documents, they are not specifically trained to handle dialogue like a conversational agent.\n",
    "This means that asking them questions out of the blue may not always be the most effective way to communicate with them. But fear not! There are two alternative approaches that can help you get the most out of these models.\n",
    "\n",
    "Firstly, you can provide some context for the model by using question and answer tags. By doing this, you can help the model understand the context in which the question is being asked, which can improve the accuracy of its responses. Think of it like providing some background information before asking a question, so the model has a better idea of what you're asking.\n",
    "The second approach is to rephrase your queries to sound more like you're writing a document, rather than engaging in a conversation. This can help give the model more context to work with, which can help it generate more accurate and useful responses. So instead of asking \"What's the weather like today?\", you could say \"Can you provide information on today's weather conditions?\" \n",
    "\n",
    "Large language models are incredibly powerful that can help us make sense of vast amounts of data and information. However, communicating with them in the same way we communicate with humans may not always be the best approach. By using question and answer tags to provide context and rephrasing our queries to sound more like we're writing a document, we can get the most out of these models and unlock new possibilities in natural language processing. So don't be afraid to get creative with your communication methods, and who knows what insights you might uncover?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5449481",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are viable strategies to improve performance of large language models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Given the following document, answer to questions using **only** the information in the document.\n",
    "\n",
    "Document: \\\"{document}\\\"\n",
    "Question: \\\"{question}\\\"\n",
    "Answer:\\\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"\\n\"]\n",
    "stop_regex = re.compile(r\"(?i)(\" + \"|\".join(re.escape(word) for word in stop_words) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20127acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.create(prompt, n_tokens=100, stop_regex=stop_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215de043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.input_text + \"ðŸ¤–\" + response.completions[0].output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7f8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
